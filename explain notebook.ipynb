{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "da32a134-bb82-4363-a395-20253b2a5fae",
      "metadata": {
        "id": "da32a134-bb82-4363-a395-20253b2a5fae"
      },
      "source": [
        "### 1. Motivation\n",
        "#### What is your dataset?\n",
        "We used three datasets related to Austin Animal Center:\n",
        "- Austin_Animal_Center_Intakes_20250419.csv: records of animals entering the shelter, including intake dates and animal types.\n",
        "- Austin_Animal_Center_Outcomes_20250419.csv: outcome records of animals leaving the shelter (e.g., adoption, euthanasia).\n",
        "- shelter_geocoded_locations.csv: a manually geocoded dataset that maps official shelter addresses in Austin to their corresponding latitude and longitude coordinates, based on data from the Austin Animal Center's official shelter list.\n",
        "- neighbour.json: downloaded from https://data.austintexas.gov/.\n",
        "\n",
        "#### Why did you choose this/these particular dataset(s)?\n",
        "The intake dataset offers a rich, time-stamped record of real-world animal shelter operations. With the help of manually geocoded shelter addresses, we were able to conduct meaningful spatial analysis. This multi-angle approach allows for an accessible and engaging visual story.\n",
        "\n",
        "#### What was your goal for the end user's experience?\n",
        "We want users to understand the patterns behind stray animal intake: when animals enter shelters most frequently, and which shelter regions are under more pressure. The visualizations should guide users from temporal overview to regional insights."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a85fb19-e337-47da-8844-a09367967daa",
      "metadata": {
        "id": "8a85fb19-e337-47da-8844-a09367967daa"
      },
      "source": [
        "### 2. Basic stats"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Cleaning & Preprocessing\n",
        "- Merged intakeâ€¯&â€¯outcome tables: According to Animal ID.Keeping duplicates for repeat visits.\n",
        "- Date parsing: Converted intake_datetime to proper datetime format.\n",
        "- Time aggregation: Grouped intake counts by month and by shelter region.\n",
        "- Outliers: Removed records with missing animal type or unmatchable shelter data."
      ],
      "metadata": {
        "id": "Ekr4MqXMVOby"
      },
      "id": "Ekr4MqXMVOby"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# è¯»å– Intakes å’Œ Outcomes æ•°æ®é›†\n",
        "intake_df = pd.read_csv(\"Austin_Animal_Center_Intakes_20250419.csv\")\n",
        "outcome_df = pd.read_csv(\"Austin_Animal_Center_Outcomes_20250419.csv\")\n",
        "\n",
        "def print_basic_info(df, name):\n",
        "    print(f\"datasetname: {name}\")\n",
        "    print(f\"datavolume: {df.shape}\")\n",
        "    print(\"\\n column name\")\n",
        "    print(df.columns.tolist())\n",
        "    print(\"\\n missing value\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\n sample\")\n",
        "    print(df.head(5))\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# æ‰“å° Intakes æ•°æ®åŸºæœ¬ä¿¡æ¯\n",
        "print_basic_info(intake_df, \"Intakes\")\n",
        "\n",
        "# æ‰“å° Outcomes æ•°æ®åŸºæœ¬ä¿¡æ¯\n",
        "print_basic_info(outcome_df, \"Outcomes\")\n",
        "\n",
        "# æå– ID é›†åˆ\n",
        "intake_ids = set(intake_df[\"Animal ID\"])\n",
        "outcome_ids = set(outcome_df[\"Animal ID\"])\n",
        "\n",
        "# æ£€æŸ¥åŒ…å«å…³ç³»\n",
        "only_in_intake = intake_ids - outcome_ids\n",
        "only_in_outcome = outcome_ids - intake_ids\n",
        "common_ids = intake_ids & outcome_ids\n",
        "\n",
        "print(f\"ğŸ¯ Intake ä¸­æ€»å…±æœ‰ ID æ•°é‡: {len(intake_ids)}\")\n",
        "print(f\"ğŸ¯ Outcome ä¸­æ€»å…±æœ‰ ID æ•°é‡: {len(outcome_ids)}\")\n",
        "print(f\"âœ… ä¸¤è¾¹å…±æœ‰çš„ ID æ•°é‡: {len(common_ids)}\")\n",
        "print(f\"âš ï¸ åªåœ¨ Intake ä¸­å‡ºç°çš„ ID: {len(only_in_intake)}\")\n",
        "print(f\"âš ï¸ åªåœ¨ Outcome ä¸­å‡ºç°çš„ ID: {len(only_in_outcome)}\")\n",
        "\n",
        "# æ˜¯å¦æœ‰é‡å¤ï¼Ÿ\n",
        "intake_duplicates = intake_df[\"Animal ID\"].duplicated().sum()\n",
        "outcome_duplicates = outcome_df[\"Animal ID\"].duplicated().sum()\n",
        "\n",
        "print(f\"\\nğŸ” Intake ä¸­é‡å¤çš„ Animal ID æ•°é‡: {intake_duplicates}\")\n",
        "print(f\"ğŸ” Outcome ä¸­é‡å¤çš„ Animal ID æ•°é‡: {outcome_duplicates}\")\n",
        "\n",
        "# æ¯ä¸ª ID åœ¨ intake ä¸­å‡ºç°çš„æ¬¡æ•°\n",
        "intake_counts = intake_df[\"Animal ID\"].value_counts()\n",
        "outcome_counts = outcome_df[\"Animal ID\"].value_counts()\n",
        "\n",
        "# æ˜¯å¦æ‰€æœ‰ ID åªå‡ºç°ä¸€æ¬¡\n",
        "print(\"Intake ä¸€å¯¹ä¸€:\", all(intake_counts == 1))\n",
        "print(\"Outcome ä¸€å¯¹ä¸€:\", all(outcome_counts == 1))\n",
        "\n",
        "# ä¿®å¤æ—¶é—´æ ¼å¼å¹¶ç»Ÿä¸€ä¸º tz-naiveï¼ˆæ— æ—¶åŒºï¼‰\n",
        "intake_df[\"DateTime\"] = pd.to_datetime(intake_df[\"DateTime\"], errors=\"coerce\").dt.tz_localize(None)\n",
        "outcome_df[\"DateTime\"] = pd.to_datetime(outcome_df[\"DateTime\"], errors=\"coerce\").dt.tz_localize(None)\n",
        "\n",
        "#å¯¹æ¯ä¸ªåŠ¨ç‰©æŒ‰æ—¶é—´æ’åºï¼Œå¹¶ç¼–å·æ¯ä¸€æ¬¡è¿›å‡º\n",
        "intake_df_sorted = intake_df.sort_values(by=[\"Animal ID\", \"DateTime\"])\n",
        "outcome_df_sorted = outcome_df.sort_values(by=[\"Animal ID\", \"DateTime\"])\n",
        "\n",
        "intake_df_sorted[\"Intake Number\"] = intake_df_sorted.groupby(\"Animal ID\").cumcount() + 1\n",
        "outcome_df_sorted[\"Outcome Number\"] = outcome_df_sorted.groupby(\"Animal ID\").cumcount() + 1\n",
        "\n",
        "#é‡å‘½åé¿å…åˆå¹¶å†²çªï¼Œä¿ç•™ MonthYear å’Œ Date of Birth\n",
        "intake_df_sorted = intake_df_sorted.rename(columns={\n",
        "    \"DateTime\": \"Intake Datetime\",\n",
        "    \"MonthYear\": \"Intake MonthYear\",\n",
        "    \"Sex upon Intake\": \"Sex upon Intake\",\n",
        "    \"Age upon Intake\": \"Age upon Intake\",\n",
        "    \"Intake Type\": \"Intake Type\",\n",
        "    \"Intake Condition\": \"Intake Condition\",\n",
        "    \"Found Location\": \"Found Location\"\n",
        "})\n",
        "\n",
        "outcome_df_sorted = outcome_df_sorted.rename(columns={\n",
        "    \"DateTime\": \"Outcome Datetime\",\n",
        "    \"MonthYear\": \"Outcome MonthYear\",\n",
        "    \"Sex upon Outcome\": \"Sex upon Outcome\",\n",
        "    \"Age upon Outcome\": \"Age upon Outcome\",\n",
        "    \"Outcome Type\": \"Outcome Type\",\n",
        "    \"Outcome Subtype\": \"Outcome Subtype\",\n",
        "    \"Date of Birth\": \"Date of Birth\"\n",
        "})\n",
        "\n",
        "#åˆå¹¶ Intake å’Œ Outcomeï¼ˆæŒ‰ç¼–å·å¯¹åº”ï¼‰\n",
        "merged_df = pd.merge(\n",
        "    intake_df_sorted,\n",
        "    outcome_df_sorted,\n",
        "    left_on=[\"Animal ID\", \"Intake Number\"],\n",
        "    right_on=[\"Animal ID\", \"Outcome Number\"],\n",
        "    how=\"left\",  # ä¿ç•™æ‰€æœ‰ Intakeï¼Œå³ä½¿æ²¡æœ‰åŒ¹é…çš„ Outcome\n",
        "    suffixes=(\"_Intake\", \"_Outcome\")\n",
        ")\n",
        "\n",
        "#å¡«å……æ²¡æœ‰ Outcome çš„è®°å½•\n",
        "merged_df[\"Outcome Number\"] = merged_df[\"Outcome Number\"].fillna(0).astype(int)\n",
        "merged_df[\"Outcome Type\"] = merged_df[\"Outcome Type\"].fillna(\"No Outcome\")\n",
        "merged_df[\"Outcome Subtype\"] = merged_df[\"Outcome Subtype\"].fillna(\"Unknown\")\n",
        "\n",
        "#æ—¶é—´åˆç†æ€§è¿‡æ»¤ï¼šå‡ºæ‰€æ—¶é—´åº”å¤§äºç­‰äºå…¥æ‰€æ—¶é—´æˆ–ä¸ºç©º\n",
        "merged_df = merged_df[\n",
        "    (merged_df[\"Outcome Datetime\"].isna()) |\n",
        "    (merged_df[\"Outcome Datetime\"] >= merged_df[\"Intake Datetime\"])\n",
        "]\n",
        "\n",
        "#åˆå¹¶å­—æ®µï¼šä¼˜å…ˆä¿ç•™ Intake æ•°æ®\n",
        "merged_df[\"Name\"] = merged_df[\"Name_Intake\"].combine_first(merged_df[\"Name_Outcome\"])\n",
        "merged_df[\"Animal Type\"] = merged_df[\"Animal Type_Intake\"].combine_first(merged_df[\"Animal Type_Outcome\"])\n",
        "merged_df[\"Breed\"] = merged_df[\"Breed_Intake\"].combine_first(merged_df[\"Breed_Outcome\"])\n",
        "merged_df[\"Color\"] = merged_df[\"Color_Intake\"].combine_first(merged_df[\"Color_Outcome\"])\n",
        "\n",
        "#å¡«å……æ²¡æœ‰å‡ºæ‰€çš„æ•°å­—å­—æ®µ\n",
        "merged_df[\"Outcome Number\"] = merged_df[\"Outcome Number\"].fillna(0).astype(int)\n",
        "\n",
        "#æ•´ç†æœ€ç»ˆå­—æ®µé¡ºåºï¼Œä¿ç•™ Intake/Outcome MonthYear å’Œ Date of Birth\n",
        "final_df = merged_df[[\n",
        "    \"Animal ID\", \"Name\", \"Animal Type\", \"Date of Birth\", \"Intake Datetime\", \"Intake Type\", \"Intake Condition\",\n",
        "    \"Intake MonthYear\", \"Found Location\", \"Outcome Datetime\", \"Outcome Type\", \"Outcome Subtype\",\n",
        "    \"Outcome MonthYear\", \"Intake Number\", \"Outcome Number\", \"Breed\", \"Color\",\n",
        "    \"Sex upon Intake\", \"Age upon Intake\", \"Sex upon Outcome\", \"Age upon Outcome\"\n",
        "]]\n",
        "\n",
        "# 11. save\n",
        "final_df.to_csv(\"updated_cleaned_merged_animal_data.csv\", index=False)\n",
        "\n",
        "#æ˜¾ç¤ºé¢„è§ˆï¼ˆå¦‚åœ¨ Jupyter ä¸­ä½¿ç”¨ï¼‰\n",
        "print(\"åˆå¹¶å®Œæˆï¼Œå…± {} æ¡è®°å½•ã€‚\".format(len(final_df)))\n",
        "final_df.head(30)\n"
      ],
      "metadata": {
        "id": "byGFZBtfmdra"
      },
      "id": "byGFZBtfmdra",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import googlemaps\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "import math\n",
        "\n",
        "# è¾“å…¥ä½ çš„ Google API å¯†é’¥\n",
        "api_key = \"AIzaSyDkZJ9NcJjGtc40JG-P3LryIwHS592ii1U\"  # æ›¿æ¢ä¸ºä½ çš„ API å¯†é’¥\n",
        "gmaps = googlemaps.Client(key=api_key)\n",
        "\n",
        "# åŠ è½½æ•°æ®æ–‡ä»¶\n",
        "file_path = 'addresses_part_1.csv'  # æ¯ä¸ªäººçš„æ–‡ä»¶ä¸åŒ\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# å‡è®¾åœ°å€åˆ—çš„åç§°æ˜¯ 'Formatted Address'\n",
        "addresses = df['Formatted Address'].dropna().unique()\n",
        "\n",
        "# åœ°ç†ç¼–ç å‡½æ•°\n",
        "def geocode_address(address):\n",
        "    try:\n",
        "        # ä½¿ç”¨ Google Maps API è¿›è¡Œåœ°ç†ç¼–ç \n",
        "        result = gmaps.geocode(address)\n",
        "        if result:\n",
        "            lat = result[0]['geometry']['location']['lat']\n",
        "            lng = result[0]['geometry']['location']['lng']\n",
        "            return address, lat, lng\n",
        "        else:\n",
        "            return address, None, None\n",
        "    except Exception as e:\n",
        "        return address, None, None\n",
        "\n",
        "# å¤šçº¿ç¨‹åœ°ç†ç¼–ç \n",
        "def geocode_addresses_in_parallel(addresses):\n",
        "    total_addresses = len(addresses)\n",
        "    progress_interval = math.ceil(total_addresses / 10)  # æ¯å¤„ç†10%çš„åœ°å€æ‰“å°ä¸€æ¬¡è¿›åº¦\n",
        "    results = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = {executor.submit(geocode_address, address): address for address in addresses}\n",
        "\n",
        "        completed = 0\n",
        "        for future in futures:\n",
        "            result = future.result()\n",
        "            results.append(result)\n",
        "            completed += 1\n",
        "            if completed % progress_interval == 0:\n",
        "                print(f\"å·²å¤„ç† {completed} / {total_addresses} åœ°å€\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# å¼€å§‹åœ°ç†ç¼–ç \n",
        "start_time = time.time()\n",
        "geocoded_data = geocode_addresses_in_parallel(addresses)\n",
        "end_time = time.time()\n",
        "\n",
        "# å°†ç»“æœè½¬ä¸º DataFrame\n",
        "geocoded_df = pd.DataFrame(geocoded_data, columns=['Address', 'Latitude', 'Longitude'])\n",
        "\n",
        "# ä¿å­˜åœ°ç†ç¼–ç ç»“æœåˆ°æ–°æ–‡ä»¶\n",
        "output_file = 'geocoded_addresses_part_1.csv'  # æ¯ä¸ªäººçš„è¾“å‡ºæ–‡ä»¶ä¸åŒ\n",
        "geocoded_df.to_csv(output_file, index=False)\n",
        "\n",
        "# æ‰“å°ç”¨æ—¶\n",
        "print(f\"åœ°ç†ç¼–ç å®Œæˆï¼Œè€—æ—¶ {end_time - start_time} ç§’ã€‚\")\n",
        "\n",
        "# æ˜¾ç¤ºç»“æœé¢„è§ˆ\n",
        "geocoded_df.head()\n",
        "\n",
        "# æä¾›æ–‡ä»¶ä¸‹è½½é“¾æ¥\n",
        "output_file"
      ],
      "metadata": {
        "id": "B8Sk2gaFvcQ3"
      },
      "id": "B8Sk2gaFvcQ3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Geocoding\n",
        "- Transfer TXT found locations to coordinates through Google Map API.\n",
        "- Shelter matching: Linked each record to a shelter location based on the shelter_name or address field.\n",
        "- Geolocation: Used shelter_geocoded_locations.csv to attach coordinates to each shelter site.\n",
        "- Neighborhood mapping: Mapped coordinates to official Austin neighborhood polygons using spatial joins."
      ],
      "metadata": {
        "id": "AJ5SQmWms8R9"
      },
      "id": "AJ5SQmWms8R9"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# è¯»å–åŸå§‹æ–‡ä»¶\n",
        "file_path = 'unique_geocoding_ready_addresses.csv'  # ä½ ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# å‡è®¾åœ°å€åˆ—ä¸º 'Formatted Address'\n",
        "addresses = df['Formatted Address'].dropna()\n",
        "\n",
        "# å°†æ•°æ®åˆ†æˆä¸¤éƒ¨åˆ†\n",
        "addresses_part_1 = addresses[:len(addresses)//2]\n",
        "addresses_part_2 = addresses[len(addresses)//2:]\n",
        "\n",
        "# å°†åˆ†å‰²çš„æ•°æ®ä¿å­˜ä¸ºä¸¤ä¸ª CSV æ–‡ä»¶\n",
        "addresses_part_1.to_csv('addresses_part_1.csv', index=False)\n",
        "addresses_part_2.to_csv('addresses_part_2.csv', index=False)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import googlemaps\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "import math\n",
        "\n",
        "# è¾“å…¥ä½ çš„ Google API å¯†é’¥\n",
        "api_key = \"AIzaSyCdpBuSe2rZT_JwVGgEYLsKGd6SAN0xm5o\"  # æ›¿æ¢ä¸ºä½ çš„ API å¯†é’¥\n",
        "gmaps = googlemaps.Client(key=api_key)\n",
        "\n",
        "# åŠ è½½æ•°æ®æ–‡ä»¶\n",
        "file_path = 'addresses_part_2.csv'  # æ¯ä¸ªäººçš„æ–‡ä»¶ä¸åŒ\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# å‡è®¾åœ°å€åˆ—çš„åç§°æ˜¯ 'Formatted Address'\n",
        "addresses = df['Formatted Address'].dropna().unique()\n",
        "\n",
        "# åœ°ç†ç¼–ç å‡½æ•°\n",
        "def geocode_address(address):\n",
        "    try:\n",
        "        # ä½¿ç”¨ Google Maps API è¿›è¡Œåœ°ç†ç¼–ç \n",
        "        result = gmaps.geocode(address)\n",
        "        if result:\n",
        "            lat = result[0]['geometry']['location']['lat']\n",
        "            lng = result[0]['geometry']['location']['lng']\n",
        "            return address, lat, lng\n",
        "        else:\n",
        "            return address, None, None\n",
        "    except Exception as e:\n",
        "        return address, None, None\n",
        "\n",
        "# å¤šçº¿ç¨‹åœ°ç†ç¼–ç \n",
        "def geocode_addresses_in_parallel(addresses):\n",
        "    total_addresses = len(addresses)\n",
        "    progress_interval = math.ceil(total_addresses / 10)  # æ¯å¤„ç†10%çš„åœ°å€æ‰“å°ä¸€æ¬¡è¿›åº¦\n",
        "    results = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = {executor.submit(geocode_address, address): address for address in addresses}\n",
        "\n",
        "        completed = 0\n",
        "        for future in futures:\n",
        "            result = future.result()\n",
        "            results.append(result)\n",
        "            completed += 1\n",
        "            if completed % progress_interval == 0:\n",
        "                print(f\"å·²å¤„ç† {completed} / {total_addresses} åœ°å€\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# å¼€å§‹åœ°ç†ç¼–ç \n",
        "start_time = time.time()\n",
        "geocoded_data = geocode_addresses_in_parallel(addresses)\n",
        "end_time = time.time()\n",
        "\n",
        "# å°†ç»“æœè½¬ä¸º DataFrame\n",
        "geocoded_df = pd.DataFrame(geocoded_data, columns=['Address', 'Latitude', 'Longitude'])\n",
        "\n",
        "# ä¿å­˜åœ°ç†ç¼–ç ç»“æœåˆ°æ–°æ–‡ä»¶\n",
        "output_file = 'geocoded_addresses_part_2.csv'  # æ¯ä¸ªäººçš„è¾“å‡ºæ–‡ä»¶ä¸åŒ\n",
        "geocoded_df.to_csv(output_file, index=False)\n",
        "\n",
        "# æ‰“å°ç”¨æ—¶\n",
        "print(f\"åœ°ç†ç¼–ç å®Œæˆï¼Œè€—æ—¶ {end_time - start_time} ç§’ã€‚\")\n",
        "\n",
        "# æ˜¾ç¤ºç»“æœé¢„è§ˆ\n",
        "geocoded_df.head()\n",
        "\n",
        "# æä¾›æ–‡ä»¶ä¸‹è½½é“¾æ¥\n",
        "output_file\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 1. è¯»å–ä¸¤ä¸ªåœ°ç†ç¼–ç éƒ¨åˆ†æ•°æ®\n",
        "df_part_1 = pd.read_csv(\"geocoded_addresses_part_1.csv\")\n",
        "df_part_2 = pd.read_csv(\"geocoded_addresses_part_2.csv\")\n",
        "\n",
        "# 2. åˆå¹¶ä¸¤ä¸ªæ•°æ®é›†\n",
        "merged_df = pd.concat([df_part_1, df_part_2], ignore_index=True)\n",
        "\n",
        "# 3. å»é‡æ“ä½œï¼Œé˜²æ­¢é‡å¤åœ°å€\n",
        "merged_df = merged_df.drop_duplicates(subset=['Address'])\n",
        "\n",
        "# 4. ä¿å­˜åˆå¹¶åçš„æ•°æ®\n",
        "merged_df.to_csv(\"geocoded_addresses_merged.csv\", index=False)\n",
        "\n",
        "print(\"åˆå¹¶å®Œæˆï¼Œå…± {} æ¡å”¯ä¸€åœ°å€è®°å½•ã€‚\".format(len(merged_df)))\n",
        "\n"
      ],
      "metadata": {
        "id": "3HuDy6JtsGf4"
      },
      "id": "3HuDy6JtsGf4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset Statistics\n",
        "- Intakes dataset rows: 126,234\n",
        "- Outcomes dataset rows: 124,513\n",
        "- Unique animal types: 5 (Dogs, Cats, Birds, Others, Small Mammals)\n",
        "- Mapped shelter coordinates: Successfully matched to all listed shelters\n",
        "- Time span: October 2013 to April 2025\n",
        "- Number of mapped shelters: 12\n",
        "- 68395 coded locations"
      ],
      "metadata": {
        "id": "t2Oc5DtTVRSm"
      },
      "id": "t2Oc5DtTVRSm"
    },
    {
      "cell_type": "markdown",
      "id": "3b4751e9-260b-4445-8819-f05f0da26df6",
      "metadata": {
        "id": "3b4751e9-260b-4445-8819-f05f0da26df6"
      },
      "source": [
        "### 3. Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dynamic Hotspot Map of Animal Intakes\n",
        "We used Foliumâ€™s HeatMapWithTime to animate changes in geographic hotspots across Austin over time. Each frame represents a specific month from October 2013 to April 2025."
      ],
      "metadata": {
        "id": "LVc-URy3OHYT"
      },
      "id": "LVc-URy3OHYT"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import folium\n",
        "from folium.plugins import HeatMapWithTime, Fullscreen, MeasureControl\n",
        "import branca.colormap as cm\n",
        "\n",
        "# === Step 1: Load data ===\n",
        "df = pd.read_csv(\"merged_data.csv\", parse_dates=[\"Intake Datetime\"])\n",
        "df = df.dropna(subset=[\"Latitude\", \"Longitude\", \"Intake Datetime\"]).copy()\n",
        "df = df[(df[\"Latitude\"].between(-90, 90)) & (df[\"Longitude\"].between(-180, 180))]\n",
        "df[\"Month_Formatted\"] = df[\"Intake Datetime\"].dt.strftime(\"%Y-%m\")\n",
        "\n",
        "# === Step 2: Add weight ===\n",
        "def add_weight(points_list):\n",
        "    from collections import defaultdict\n",
        "    point_counts = defaultdict(int)\n",
        "    weighted = []\n",
        "    for p in points_list:\n",
        "        rounded = (round(p[0], 3), round(p[1], 3))\n",
        "        point_counts[rounded] += 1\n",
        "    for p in points_list:\n",
        "        rounded = (round(p[0], 3), round(p[1], 3))\n",
        "        weighted.append([p[0], p[1], min(point_counts[rounded] * 0.2, 1)])\n",
        "    return weighted\n",
        "\n",
        "time_index = sorted(df[\"Month_Formatted\"].unique())\n",
        "heat_data = [add_weight(df[df[\"Month_Formatted\"] == m][[\"Latitude\", \"Longitude\"]].values.tolist()) for m in time_index]\n",
        "\n",
        "# === Step 3: Setup map ===\n",
        "center = [df[\"Latitude\"].median(), df[\"Longitude\"].median()]\n",
        "m = folium.Map(location=center, zoom_start=12, tiles=\"CartoDB Positron\", control_scale=True)\n",
        "MeasureControl(position=\"bottomright\").add_to(m)\n",
        "\n",
        "# === Step 4: Color gradient ===\n",
        "colors = [\"#c4e9f2\", \"#7fbbdd\", \"#f58b05\", \"#ffc22f\"]\n",
        "colormap = cm.LinearColormap(colors=colors, index=[0, 0.3, 0.6, 1], vmin=0, vmax=1, caption=\"Hotspot Density\")\n",
        "colormap.add_to(m)\n",
        "\n",
        "# === Step 5: Heatmap with time ===\n",
        "HeatMapWithTime(\n",
        "    data=heat_data,\n",
        "    index=time_index,\n",
        "    radius=15,\n",
        "    min_opacity=0.3,\n",
        "    max_opacity=0.9,\n",
        "    gradient={i / 3: colors[i] for i in range(4)},\n",
        "    use_local_extrema=True,\n",
        "    auto_play=True,\n",
        "    display_index=True\n",
        ").add_to(m)\n",
        "\n",
        "# === Step 6: Title box ===\n",
        "title_html = f\"\"\"\n",
        "<div id=\"title-card\" style=\"\n",
        "    position: absolute;\n",
        "    top: 20px;\n",
        "    left: 20px;\n",
        "    z-index: 9999;\n",
        "    background-color: rgba(255,255,255,0.85);\n",
        "    padding: 10px 15px;\n",
        "    border-radius: 8px;\n",
        "    font-family: sans-serif;\n",
        "    box-shadow: 0 0 8px rgba(0,0,0,0.1);\n",
        "    max-width: 300px;\n",
        "    font-size: 13px;\n",
        "\">\n",
        "    <h4 style=\"margin: 0; font-size: 1em;\"><b>Dynamic Hotspot Map of Animal Intakes</b></h4>\n",
        "    <p style=\"margin: 2px 0;\">Monthly distribution of animal intake hotspots</p>\n",
        "    <p style=\"margin: 0;\">Data range: {df[\"Intake Datetime\"].min().strftime('%Y-%m')} to {df[\"Intake Datetime\"].max().strftime('%Y-%m')}</p>\n",
        "</div>\n",
        "\"\"\"\n",
        "m.get_root().html.add_child(folium.Element(title_html))\n",
        "\n",
        "# === Step 7: Final JS fix using MutationObserver ===\n",
        "custom_js = \"\"\"\n",
        "<script>\n",
        "document.addEventListener(\"DOMContentLoaded\", function () {\n",
        "    const map = document.querySelector('.folium-map');\n",
        "    if (map) map.style.position = 'relative';\n",
        "\n",
        "    const observer = new MutationObserver(() => {\n",
        "        const ctrl = document.querySelector('.leaflet-control-timecontrol');\n",
        "        const target = document.querySelector('.leaflet-bottom.leaflet-left');\n",
        "\n",
        "        if (ctrl && target && ctrl.querySelectorAll(\"button\").length > 0) {\n",
        "            // Remove all buttons except play/pause (index 1)\n",
        "            const buttons = ctrl.querySelectorAll(\"button\");\n",
        "            buttons.forEach((btn, i) => {\n",
        "                if (i !== 1) btn.remove();\n",
        "            });\n",
        "\n",
        "            // Remove fps control row\n",
        "            const rows = ctrl.querySelectorAll(\"tr\");\n",
        "            if (rows.length > 1) rows[1].remove();\n",
        "\n",
        "            // Move and style control\n",
        "            target.appendChild(ctrl);\n",
        "            Object.assign(ctrl.style, {\n",
        "                position: 'absolute',\n",
        "                left: '0',\n",
        "                bottom: '0',\n",
        "                margin: '20px',\n",
        "                zIndex: '10000',\n",
        "                background: 'white',\n",
        "                borderRadius: '8px',\n",
        "                padding: '8px',\n",
        "                boxShadow: '0 0 6px rgba(0,0,0,0.2)',\n",
        "                display: 'inline-block',\n",
        "                maxWidth: '600px',\n",
        "                overflow: 'hidden',\n",
        "                transform: 'translateX(0%)'\n",
        "            });\n",
        "\n",
        "            observer.disconnect();\n",
        "        }\n",
        "    });\n",
        "\n",
        "    observer.observe(document.body, { childList: true, subtree: true });\n",
        "});\n",
        "</script>\n",
        "\"\"\"\n",
        "m.get_root().html.add_child(folium.Element(custom_js))\n",
        "\n",
        "# === Step 8: Fullscreen button ===\n",
        "Fullscreen(position=\"topright\").add_to(m)\n",
        "\n",
        "# === Step 9: Save output ===\n",
        "m.save(\"heatmap_final_clean_controls.html\")\n",
        "print(\"âœ… Saved: heatmap_final_clean_controls.html\")"
      ],
      "metadata": {
        "id": "36XTPLsZOMGK"
      },
      "id": "36XTPLsZOMGK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Choropleth of Animal Intakes\n",
        "Using a GeoJSON file of Austin neighborhoods, we mapped total intake counts per region and applied a yellow-to-blue gradient using Folium and D3 for visual contrast."
      ],
      "metadata": {
        "id": "-8JjHdjOOoDt"
      },
      "id": "-8JjHdjOOoDt"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import folium\n",
        "import json\n",
        "from shapely.geometry import shape, Point\n",
        "import branca.colormap as cm\n",
        "\n",
        "# === Step 1: åŠ è½½æ”¶å®¹æ•°æ® ===\n",
        "df = pd.read_csv(\"merged_data.csv\", parse_dates=[\"Intake Datetime\"])\n",
        "df = df.dropna(subset=[\"Latitude\", \"Longitude\"])\n",
        "\n",
        "# === Step 2: åŠ è½½ GeoJSON æ•°æ® ===\n",
        "with open(\"Neighborhoods_20250506.geojson\") as f:\n",
        "    gj = json.load(f)\n",
        "\n",
        "# è‡ªåŠ¨æ£€æµ‹åŒºåŸŸå­—æ®µå\n",
        "region_field = next((k for k in gj[\"features\"][0][\"properties\"]\n",
        "                     if \"name\" in k.lower() or \"label\" in k.lower()),\n",
        "                    list(gj[\"features\"][0][\"properties\"].keys())[0])\n",
        "\n",
        "# === Step 3: å°†ç‚¹åŒ¹é…åˆ°åŒºåŸŸ ===\n",
        "polygon_map = {f[\"properties\"][region_field]: shape(f[\"geometry\"]) for f in gj[\"features\"]}\n",
        "\n",
        "def assign_region(row):\n",
        "    pt = Point(row[\"Longitude\"], row[\"Latitude\"])\n",
        "    for name, poly in polygon_map.items():\n",
        "        if poly.contains(pt):\n",
        "            return name\n",
        "    return None\n",
        "\n",
        "df[\"region\"] = df.apply(assign_region, axis=1)\n",
        "df = df.dropna(subset=[\"region\"])\n",
        "\n",
        "# === Step 4: æ±‡æ€»æ¯ä¸ªåŒºåŸŸçš„æ”¶å®¹æ•°é‡ ===\n",
        "region_counts = df.groupby(\"region\").size().reset_index(name=\"animal_count\")\n",
        "region_dict = region_counts.set_index(\"region\")[\"animal_count\"].to_dict()\n",
        "\n",
        "# å†™å…¥ GeoJSON å±æ€§\n",
        "for f in gj[\"features\"]:\n",
        "    rid = f[\"properties\"].get(region_field)\n",
        "    count = region_dict.get(rid, 0)\n",
        "    f[\"properties\"][\"animal_count\"] = int(count)\n",
        "\n",
        "# === Step 5: åˆ›å»ºåº•å›¾ ===\n",
        "m = folium.Map(location=[30.27, -97.74], zoom_start=11, tiles=\"CartoDB Positron\")\n",
        "\n",
        "# === Step 6: åˆ›å»ºæ¸å˜è‰²æ¡ï¼ˆæ·¡é»„åˆ°ä¸»è“ï¼‰===\n",
        "colormap = cm.LinearColormap(\n",
        "    colors=[\"#FFF5BF\", \"#639BFF\"],  # æ·¡é»„ â†’ ä¸»è“\n",
        "    vmin=min(region_dict.values()),\n",
        "    vmax=max(region_dict.values()),\n",
        "    caption=\"Total Animal Intakes by Neighborhood\"\n",
        ")\n",
        "colormap.add_to(m)\n",
        "\n",
        "# === Step 7: ç»˜åˆ¶ GeoJson å¤šè¾¹å½¢å›¾å±‚ ===\n",
        "folium.GeoJson(\n",
        "    gj,\n",
        "    style_function=lambda feature: {\n",
        "        \"fillColor\": colormap(feature[\"properties\"].get(\"animal_count\", 0)),\n",
        "        \"color\": \"#A5C8FF\",         # æµ…è“è¾¹ç•Œ\n",
        "        \"weight\": 0.5,\n",
        "        \"fillOpacity\": 0.7\n",
        "    },\n",
        "    tooltip=folium.GeoJsonTooltip(\n",
        "        fields=[region_field, \"animal_count\"],\n",
        "        aliases=[\"Neighborhood:\", \"Animal Intakes:\"],\n",
        "        localize=True,\n",
        "        style=(\n",
        "            \"background-color: #FFF5BF; \"\n",
        "            \"color: #3C3C3C; \"\n",
        "            \"font-family: Comic Sans MS, sans-serif; \"\n",
        "            \"font-size: 12px; \"\n",
        "            \"padding: 6px; border-radius: 5px;\"\n",
        "        )\n",
        "    )\n",
        ").add_to(m)\n",
        "\n",
        "# === Step 8: æ·»åŠ æ ‡é¢˜å¡ç‰‡ ===\n",
        "title_html = \"\"\"\n",
        "<div style=\"\n",
        "    position: absolute;\n",
        "    top: 20px;\n",
        "    left: 20px;\n",
        "    z-index: 9999;\n",
        "    background-color: #FFFAF0;\n",
        "    padding: 10px 15px;\n",
        "    border-radius: 10px;\n",
        "    font-family: Comic Sans MS, sans-serif;\n",
        "    box-shadow: 0 0 6px rgba(0,0,0,0.1);\n",
        "    color: #3C3C3C;\n",
        "    font-size: 13px;\n",
        "    max-width: 300px;\n",
        "\">\n",
        "    <h4 style=\"margin: 0; font-size: 16px;\"><b>Choropleth of Animal Intakes</b></h4>\n",
        "    <p style=\"margin: 4px 0;\">Neighborhoods shaded by total intake counts</p>\n",
        "    <p style=\"margin: 0;\">Data source: merged_data.csv</p>\n",
        "</div>\n",
        "\"\"\"\n",
        "m.get_root().html.add_child(folium.Element(title_html))\n",
        "\n",
        "# === Step 9: å¯¼å‡ºä¸º HTML æ–‡ä»¶ ===\n",
        "m.save(\"choropleth_yellow_to_blue.html\")\n",
        "print(\"âœ… Saved: choropleth_yellow_to_blue.html\")"
      ],
      "metadata": {
        "id": "Kak7YQbAPcE_"
      },
      "id": "Kak7YQbAPcE_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Monthly shelter-intake trend\n",
        "We grouped the dataset by intake month and counted total animal entries. A time series chart was created using Plotly to reflect seasonal and long-term trends."
      ],
      "metadata": {
        "id": "HOhyMYXjPlVs"
      },
      "id": "HOhyMYXjPlVs"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# === Step 1: åŠ è½½ intake æ•°æ® ===\n",
        "df = pd.read_csv(\"merged_data.csv\", parse_dates=[\"Intake Datetime\"])\n",
        "df = df.dropna(subset=[\"Intake Datetime\"])\n",
        "df[\"Month\"] = df[\"Intake Datetime\"].dt.to_period(\"M\").astype(str)\n",
        "\n",
        "monthly_counts = (\n",
        "    df.groupby(\"Month\")\n",
        "    .size()\n",
        "    .reset_index(name=\"Count\")\n",
        "    .sort_values(\"Month\")\n",
        ")\n",
        "monthly_counts[\"Month\"] = pd.to_datetime(monthly_counts[\"Month\"])\n",
        "\n",
        "# === Step 2: è‡ªå®šä¹‰é¢œè‰²æ ·å¼ ===\n",
        "colors = {\n",
        "    \"background\": \"#FFFAF0\",   # å¥¶ç™½è‰²\n",
        "    \"line\": \"#639BFF\",         # æŠ˜çº¿è“\n",
        "    \"marker\": \"#FFE25F\",       # æŸ æª¬é»„ç‚¹\n",
        "    \"font\": \"#3C3C3C\"          # æ·±ç°å­—ä½“\n",
        "}\n",
        "\n",
        "# === Step 3: æ„å»ºå›¾è¡¨ ===\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=monthly_counts[\"Month\"],\n",
        "    y=monthly_counts[\"Count\"],\n",
        "    mode=\"lines+markers\",\n",
        "    line=dict(color=colors[\"line\"], width=4, shape=\"spline\"),\n",
        "    marker=dict(size=9, color=colors[\"marker\"], line=dict(width=1, color=\"white\")),\n",
        "    hovertemplate=\"Month: %{x|%b %Y}<br>Intakes: %{y}<extra></extra>\"\n",
        "))\n",
        "\n",
        "# === Step 4: å¸ƒå±€ä¸é£æ ¼ ===\n",
        "fig.update_layout(\n",
        "    title=\"From Streets to Shelter: Monthly Intake Trend\",\n",
        "    xaxis_title=\"Month\",\n",
        "    yaxis_title=\"Number of Animals\",\n",
        "    template=\"simple_white\",\n",
        "    font=dict(family=\"Comic Sans MS, Arial, sans-serif\", size=16, color=colors[\"font\"]),\n",
        "    hovermode=\"x unified\",\n",
        "    margin=dict(t=60, l=60, r=40, b=60),\n",
        "    plot_bgcolor=colors[\"background\"],\n",
        "    paper_bgcolor=colors[\"background\"]\n",
        ")\n",
        "\n",
        "# === Step 5: å¯¼å‡ºä¸º HTML æ–‡ä»¶ ===\n",
        "fig.write_html(\"monthly_intake_lemon_highlight_cartoon.html\")\n",
        "print(\"âœ… Saved: monthly_intake_lemon_highlight_cartoon.html\")"
      ],
      "metadata": {
        "id": "Ivg9gATUPv3Z"
      },
      "id": "Ivg9gATUPv3Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Shelter Coverage & Intakes\n",
        "We made this map to show where each animal shelter can currently reach within 5â€¯km and where most animals are being taken in. This helps us quickly spot places that still lack nearby shelter support, so we know where to add new shelters or send extra help."
      ],
      "metadata": {
        "id": "7EEUxRkzn5wC"
      },
      "id": "7EEUxRkzn5wC"
    },
    {
      "cell_type": "code",
      "source": [
        "import json, warnings\n",
        "import pandas as pd, geopandas as gpd\n",
        "from shapely.geometry import shape\n",
        "import folium, branca\n",
        "from folium.plugins import MarkerCluster\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# basics\n",
        "NEIGH_JSON  = \"neighbour.json\"\n",
        "ANIMAL_CSV  = \"merged_data.csv\"\n",
        "SHELTER_CSV = \"updated_shelter_geocoded_locations.csv\"\n",
        "ICON_PATH   = \"rounded_shelter_icon.png\"\n",
        "OUTCOME_COL      = \"Outcome Type\"\n",
        "ADOPTED_VALUE    = \"Adoption\"\n",
        "\n",
        "\n",
        "CIRCLE_RADIUS = 5000          # shelter radius (m)\n",
        "MAX_ANIMALS   = 6000          # sampling cap\n",
        "COLOR_MIN, COLOR_MAX, N_BINS = \"#fff5bf\", \"#92b7ec\", 7\n",
        "\n",
        "#  è¯»å– neighbourhood.json\n",
        "def load_neighbourhood(path=NEIGH_JSON):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw = json.load(f)\n",
        "    return gpd.GeoDataFrame(\n",
        "        [{\"geometry\": shape(feat[\"the_geom\"]),\n",
        "          **{k: v for k, v in feat.items() if k != \"the_geom\"}} for feat in raw],\n",
        "        crs=\"EPSG:4326\")\n",
        "\n",
        "gdf_neigh = load_neighbourhood()\n",
        "name_col = next(c for c in [\"neighname\",\"NAME\",\"name\",\"neighborhood\"] if c in gdf_neigh)\n",
        "\n",
        "# è¯»å–åŠ¨ç‰© & shelters\n",
        "animals  = pd.read_csv(ANIMAL_CSV).dropna(subset=[\"Latitude\",\"Longitude\"])\n",
        "if len(animals) > MAX_ANIMALS:\n",
        "    animals = animals.sample(MAX_ANIMALS, random_state=42)\n",
        "shelters = pd.read_csv(SHELTER_CSV)\n",
        "\n",
        "# åŠ¨ç‰© â†’ ç¤¾åŒºåŒ¹é…\n",
        "gdf_animals = gpd.GeoDataFrame(\n",
        "    animals, geometry=gpd.points_from_xy(animals.Longitude, animals.Latitude), crs=\"EPSG:4326\")\n",
        "joined = gpd.sjoin(gdf_animals, gdf_neigh[[name_col,\"geometry\"]],\n",
        "                   predicate=\"within\", how=\"left\")\n",
        "cnts = joined.groupby(name_col)[\"Animal ID\"].count().rename(\"animal_cnt\").reset_index()\n",
        "gdf_neigh = gdf_neigh.merge(cnts, on=name_col, how=\"left\").fillna({\"animal_cnt\":0})\n",
        "\n",
        "# colour\n",
        "bins = pd.qcut(gdf_neigh[\"animal_cnt\"], N_BINS, duplicates=\"drop\", retbins=True)[1]\n",
        "cm = branca.colormap.LinearColormap([COLOR_MIN,COLOR_MAX],\n",
        "                                    vmin=bins.min(), vmax=bins.max()\n",
        "                                    ).to_step(len(bins)-1)\n",
        "cm.caption = \"Total Animal Intakes by Neighborhood\"\n",
        "def style_fn(f):\n",
        "    return {\"fillColor\": cm(f[\"properties\"][\"animal_cnt\"]),\n",
        "            \"color\":\"#A5C8FF\",\"weight\":0.6,\"fillOpacity\":0.7}\n",
        "\n",
        "#Folium åœ°å›¾\n",
        "m = folium.Map(location=[30.27,-97.74], zoom_start=11, tiles=\"cartodbpositron\")\n",
        "\n",
        "folium.GeoJson(\n",
        "    gdf_neigh.to_json(), style_function=style_fn,\n",
        "    tooltip=folium.GeoJsonTooltip(\n",
        "        fields=[name_col,\"animal_cnt\"],\n",
        "        aliases=[\"Neighborhood\",\"Total Intakes\"],\n",
        "        sticky=False, labels=True,\n",
        "        style=(\"background:#FFF5BF;font-family:'Comic Sans MS';\"\n",
        "               \"padding:4px;border-radius:5px;font-size:12px;\"))\n",
        "    ,name=\"Intake Choropleth\").add_to(m)\n",
        "cm.add_to(m)\n",
        "\n",
        "# Info card\n",
        "m.get_root().html.add_child(branca.element.Element(f\"\"\"\n",
        "<div style=\"position:absolute;top:20px;left:20px;z-index:9999;\n",
        "            background:#FFFAF0;padding:10px 15px;border-radius:10px;\n",
        "            font-family:'Comic Sans MS',sans-serif;font-size:13px;color:#3C3C3C;\n",
        "            box-shadow:0 0 6px rgba(0,0,0,0.1);max-width:320px;\">\n",
        "  <h4 style=\"margin:0;font-size:16px;\"><b>Shelter Coverage & Intakes</b></h4>\n",
        "  <p style=\"margin:4px 0;\">Base color: intake counts (yellowâ†’blue)<br/>\n",
        "     Blue circle: {CIRCLE_RADIUS//1000}&nbsp;km shelter radius<br/>\n",
        "     House: shelter location</p>\n",
        "</div>\"\"\"))\n",
        "\n",
        "\n",
        "\n",
        "# MarkerCluster color\n",
        "cluster = MarkerCluster(\n",
        "    name='Stray Animals (cluster)',\n",
        "    icon_create_function=r'''\n",
        "    function(cluster){\n",
        "        var count = cluster.getChildCount();\n",
        "        var color = '#FFA534';          // â‰¤25  orange\n",
        "        if (count > 100)      { color = '#A3C4F3'; }   // >100  pastel blue\n",
        "        else if (count > 25)  { color = '#FFE599'; }   // 26â€‘100 pastel yellow\n",
        "\n",
        "        return new L.DivIcon({\n",
        "            html: '<div style=\"background:'+color+';\"><span>'+count+'</span></div>',\n",
        "            className: 'custom-cluster',\n",
        "            iconSize: [40, 40]\n",
        "        });\n",
        "    }\n",
        "    '''\n",
        ").add_to(m)\n",
        "\n",
        "# æŠŠåŠ¨ç‰©ç‚¹åŠ å…¥èšç±»ï¼ˆä¿æŒä¸å˜ï¼‰\n",
        "for _, r in animals.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[r.Latitude, r.Longitude],\n",
        "        radius=3,\n",
        "        color='#3186CC',\n",
        "        fill=True,\n",
        "        fill_color='#3186CC',\n",
        "        fill_opacity=0.5,\n",
        "        popup=f\"{r['Animal Type']} â€” {r['Found Location']}\"\n",
        "    ).add_to(cluster)\n",
        "\n",
        "# è¿½åŠ èšç±»æ°”æ³¡çš„ CSSï¼ˆæ•°å­—å±…ä¸­ã€åœ†å½¢ï¼‰\n",
        "m.get_root().html.add_child(branca.element.Element(\"\"\"\n",
        "<style>\n",
        ".custom-cluster div{\n",
        "  width:40px; height:40px; line-height:40px; border-radius:20px;\n",
        "  text-align:center; color:#FFFFFF; font-weight:bold;\n",
        "  font-family:Arial,Helvetica,sans-serif;\n",
        "  box-shadow:0 0 0 1.5px #fff inset;\n",
        "}\n",
        "</style>\n",
        "\"\"\"))\n",
        "\n",
        "\n",
        "# Individual points\n",
        "COLOR_MAP = {\n",
        "    \"Dog\":   \"#FFA534\",\n",
        "    \"Cat\":   \"#FFE599\",\n",
        "    \"Other\": \"#A3C4F3\"\n",
        "}\n",
        "DEFAULT_COLOR = \"#8CD5FF\"\n",
        "\n",
        "# FeatureGroup\n",
        "type_groups = {}\n",
        "for typ in animals[\"Animal Type\"].unique():\n",
        "    fg = folium.FeatureGroup(name=f\"{typ} (points)\", show=True)\n",
        "    fg.add_to(m)\n",
        "    type_groups[typ] = fg\n",
        "\n",
        "# add points to groups\n",
        "for _, r in animals.iterrows():\n",
        "    typ = r[\"Animal Type\"]\n",
        "    fg  = type_groups.get(typ)\n",
        "    if fg is None:                       # CSV é‡Œå‡ºç°æœªçŸ¥æ–°ç±»å‹\n",
        "        fg = type_groups.setdefault(\"Other\", folium.FeatureGroup(\n",
        "                name=\"Other (points)\", show=True).add_to(m))\n",
        "    color = COLOR_MAP.get(typ, DEFAULT_COLOR)\n",
        "    folium.CircleMarker(\n",
        "        location=[r.Latitude, r.Longitude],\n",
        "        radius=3.5,\n",
        "        color=color, weight=0.5,\n",
        "        fill=True, fill_color=color, fill_opacity=0.35,\n",
        "        popup=f\"{typ} â€” {r['Found Location']}\"\n",
        "    ).add_to(fg)\n",
        "\n",
        "#Shelters & Coverage\n",
        "\n",
        "\n",
        "shelter_fg = folium.FeatureGroup(name=\"Shelters & Coverage\", show=True)\n",
        "\n",
        "for _, r in shelters.iterrows():\n",
        "    # è¦†ç›–åœˆ\n",
        "    folium.Circle(\n",
        "        location=[r.latitude, r.longitude],\n",
        "        radius=CIRCLE_RADIUS,\n",
        "        color=\"#629BFD\", weight=1,\n",
        "        fill=True, fill_color=\"#629BFD\", fill_opacity=0.25,\n",
        "        tooltip=r.shelter_name\n",
        "    ).add_to(shelter_fg)\n",
        "\n",
        "    folium.Marker(\n",
        "        location=[r.latitude, r.longitude],\n",
        "        icon=folium.CustomIcon(ICON_PATH, icon_size=(34, 34)),\n",
        "        tooltip=r.shelter_name\n",
        "    ).add_to(shelter_fg)\n",
        "\n",
        "# **å…³é”®**ï¼šæœ€åå†æŠŠ FeatureGroup åŠ åˆ°åœ°å›¾ï¼Œè¿™æ ·å®ƒåœ¨æœ€ä¸Šå±‚\n",
        "shelter_fg.add_to(m)\n",
        "\n",
        "\n",
        "folium.LayerControl(position=\"topright\", collapsed=False).add_to(m)\n",
        "\n",
        "m.save(\"shelter_and_animal_intakes.html\")\n",
        "print(\"Map saved: shelter_and_animal_intakes.html\")"
      ],
      "metadata": {
        "id": "w1cxH_8co9-H"
      },
      "id": "w1cxH_8co9-H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Intake â†’ Outcome Flow\n",
        "We made this Sankey chart to show where most animals enter the system and where they finally go, so we can see how well the main rescueâ€‘toâ€‘placement pipeline works and which small streams still need help."
      ],
      "metadata": {
        "id": "eSegtOiWqSlG"
      },
      "id": "eSegtOiWqSlG"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, plotly.graph_objects as go\n",
        "\n",
        "df = pd.read_csv(\"merged_data.csv\")\n",
        "\n",
        "src_col = \"Intake Type\"\n",
        "tgt_col = \"Outcome Type\"\n",
        "\n",
        "flow = (df.groupby([src_col, tgt_col])\n",
        "          .size()\n",
        "          .reset_index(name=\"count\"))\n",
        "\n",
        "#  å»ºç«‹ labelâ‡„ç´¢å¼•æ˜ å°„\n",
        "labels = pd.concat([flow[src_col], flow[tgt_col]]).unique().tolist()\n",
        "label2id = {lbl:i for i, lbl in enumerate(labels)}\n",
        "\n",
        "flow[\"src_id\"] = flow[src_col].map(label2id)\n",
        "flow[\"tgt_id\"] = flow[tgt_col].map(label2id)\n",
        "\n",
        "# 3) Sankey\n",
        "src_pal = ['#f9cb9c', '#ffe599']\n",
        "tgt_color = '#cfe2f3'\n",
        "\n",
        "# åˆ¤æ–­å“ªäº›æ˜¯æºèŠ‚ç‚¹\n",
        "src_set = set(flow[src_col])\n",
        "\n",
        "# æ„é€ èŠ‚ç‚¹é¢œè‰²\n",
        "node_colors = []\n",
        "src_color_map = {}\n",
        "for i, lbl in enumerate(labels):\n",
        "    if lbl in src_set:\n",
        "        c = src_pal[i % 2]         # äº¤æ›¿é€‰è‰²\n",
        "        src_color_map[lbl] = c\n",
        "        node_colors.append(c)\n",
        "    else:\n",
        "        node_colors.append(tgt_color)\n",
        "\n",
        "# #RRGGBB â†’ rgba(r,g,b,Î±)\n",
        "def hex2rgba(hexclr, alpha=0.4):\n",
        "    hexclr = hexclr.lstrip('#')\n",
        "    r, g, b = (int(hexclr[j:j+2], 16) for j in (0,2,4))\n",
        "    return f\"rgba({r},{g},{b},{alpha})\"\n",
        "\n",
        "link_colors = [\n",
        "    hex2rgba(src_color_map[ labels[s] ]) for s in flow[\"src_id\"]\n",
        "]\n",
        "\n",
        "\n",
        "fig = go.Figure(go.Sankey(\n",
        "    node=dict(\n",
        "        label     = labels,\n",
        "        pad       = 20,\n",
        "        thickness = 18,\n",
        "        color     = node_colors\n",
        "    ),\n",
        "    link=dict(\n",
        "        source = flow[\"src_id\"],\n",
        "        target = flow[\"tgt_id\"],\n",
        "        value  = flow[\"count\"],\n",
        "        color  = link_colors\n",
        "    )\n",
        "))\n",
        "fig.update_layout(\n",
        "    title_text = \"Austin Animal Intake â†’ Outcome Flow (2014â€‘2024)\",\n",
        "    font_size  = 12, height = 500,width=860,\n",
        "    font_family='Comic Sans MS',\n",
        "    margin     = dict(l=10, r=10, t=40, b=10),\n",
        "    paper_bgcolor= \"rgba(0,0,0,0)\",   # â† æ•´å¼ ç”»å¸ƒé€æ˜\n",
        "    plot_bgcolor = \"rgba(0,0,0,0)\",    # â† ç»˜å›¾åŒºé€æ˜\n",
        "    autosize=False\n",
        ")\n",
        "fig.write_html(\n",
        "    \"animal_outcomes_flow_sankey.html\",\n",
        "    include_plotlyjs=\"cdn\",\n",
        "    full_html=False,\n",
        "    config={\"responsive\": True}\n",
        ")\n",
        "print(\"Sankey saved\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4lLWCR0IqjtO"
      },
      "id": "4lLWCR0IqjtO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "975f7071-f600-47a1-a4af-27239609719d",
      "metadata": {
        "id": "975f7071-f600-47a1-a4af-27239609719d"
      },
      "source": [
        "### 4. Genre\n",
        "#### Which genre of data story did you use?\n",
        "We used a Martini Glass narrative structure: The stem is a focused, linear presentation (time series â†’ heatmap animation),followed by an open exploration body (static choropleth map for spatial analysis and comparison).\n",
        "\n",
        "#### Visual Narrative tools used\n",
        "- Graphical highlighting (e.g., saturation of heatmap points over time)\n",
        "- Progressive reveal (the animated heatmap adds time dimension interactively)\n",
        "- Visual grouping (e.g., color mapping in the choropleth for comparing regions)\n",
        "\n",
        "#### Narrative Structure tools used\n",
        "- Author-driven sequence at the start (clear framing of the problem through time trend)\n",
        "- Reader-driven interaction in choropleth map and animation slider\n",
        "- Multi-messaging via annotation blocks and map legends"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0bd7ee9-6b16-4959-b980-e9f821ec6015",
      "metadata": {
        "id": "b0bd7ee9-6b16-4959-b980-e9f821ec6015"
      },
      "source": [
        "###  5. Visualizations\n",
        "To effectively communicate insights from the dataset, we developed a set of visualizations, each tailored to a specific narrative need:\n",
        "\n",
        "We began with an animated heatmap, built using Foliumâ€™s HeatMapWithTime, which dynamically displays how the geographic concentration of stray animal pickups evolved over time. This animation reveals consistent hotspots in downtown and east Austin, and shows how stray activity gradually expanded northeast over the years. It adds a powerful spatio-temporal dimension to the narrative, guiding the viewer through changing urban shelter pressures.\n",
        "\n",
        "To offer a cumulative view of shelter burden, we created a neighborhood-level choropleth map using Folium and GeoJSON. The map uses a yellow-to-blue color gradient to represent total intake counts per region. Areas like East Riverside and St. John clearly stand out, allowing readers to compare neighborhood-level disparities in animal intake.\n",
        "\n",
        "We then introduced a monthly time series line chart, constructed with Plotly, to illustrate seasonal trends in shelter intakes from 2013 to 2025."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780e5fb2-e114-49de-b769-77c22e1d176c",
      "metadata": {
        "id": "780e5fb2-e114-49de-b769-77c22e1d176c"
      },
      "source": [
        "### 6. Discussion\n",
        "#### What went well?\n",
        "This project provided a rich opportunity to explore stray animal intake patterns in Austin by combining spatial, temporal, and categorical data through a carefully structured visual narrative.\n",
        "Our use of the Martini Glass structure (Segel & Heer, 2010) was especially helpful in guiding the viewer. The linear sequence of visualizations at the beginning ensured a clear introduction to the problem. As the narrative progressed, we allowed more room for exploration â€” such as through the interactive intake-condition outcome chart and the Sankey diagram. These elements supported both author-driven messaging and reader-driven discovery, making the overall experience engaging and accessible\n",
        "\n",
        "\n",
        "#### What is still missing? What could be improved? Why?\n",
        "Despite these strengths, several limitations remain. Some of our charts were static, especially the bar and pie charts, which could have been more engaging if built with interactive tools like Plotly or Altair. Additionally, while we conducted extensive exploratory analysis, we did not incorporate predictive modeling. Future versions of this project could include classification models (e.g., predicting adoption likelihood based on intake condition, age, or shelter) to offer more actionable insights for shelter operations. Finally, although we used a manually geocoded list of shelter locations to assess service coverage, the spatial accuracy of some intake data may still be limited by the original address format or missing GPS points. Expanding the dataset or combining it with open civic maps could enhance precision in further iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab3ec31-4b2b-48db-85c8-1dfac627b977",
      "metadata": {
        "id": "3ab3ec31-4b2b-48db-85c8-1dfac627b977"
      },
      "source": [
        "### 7. Contributions\n",
        "- s242613 Yuling Zhai: Geocoding/Heatmap of stray-animal pickup locations/\n",
        "Neighborhood-level intake counts/Monthly shelter-intake trend\n",
        "- s242614 Shimin Huang: Data Cleaning & Preprocessing/Shelter Coverage & Intakes/Intake-Outcome Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "Segel & Heer, 2010, Narrative Visualization: Telling Stories with Data\n"
      ],
      "metadata": {
        "id": "IJ7ntjXhU1EI"
      },
      "id": "IJ7ntjXhU1EI"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}